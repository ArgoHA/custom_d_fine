project_name: vis_drone # for wandb
exp_name: baseline # experiment name

exp: ${exp_name}_${now_dir}

model_name: s # model size (n, s, m, l, x)

train:
  ### Paths ###
  root: /home/argo/Desktop/Projects/vis_drone # project root with dataset and outputs
  pretrained_dataset: coco # coco, obj2coco
  pretrained_model_path: pretrained/dfine_${model_name}_${train.pretrained_dataset}.pth # dfine_m_obj2coco.pth

  data_path: ${train.root}/data/dataset # path to dataset
  path_to_test_data: ${train.root}/VisDrone2019-DET-test-dev/images  # path to test set, used in infer script
  path_to_save: ${train.root}/output/models/${exp} # where to save output

  debug_img_path: ${train.root}/output/debug_images
  eval_preds_path: ${train.root}/output/eval_preds
  bench_img_path: ${train.root}/output/bench_imgs
  infer_path: ${train.root}/output/infer

  ### Configs ###
  use_wandb: True
  device: cuda
  label_to_name: # dataset's classes
    0: pedestrian
    1: people
    2: bicycle
    3: car
    4: van
    5: truck
    6: tricycle
    7: awning-tricycle
    8: bus
    9: motor
  use_one_class: False

  img_size: [640, 640] # (h, w)
  keep_ratio: False # image aspect ratio, if True - image will be padded
  to_visualize_eval: True # save images with gt and preds
  debug_img_processing: True # save images after preprocessing

  amp_enabled: True # use automatic mixed precision
  clip_max_norm: 0.1 # gradient clipping

  batch_size: 8 # physical, should fit on the device
  b_accum_steps: 1 # grad accumulation (n * bs)
  epochs: 55
  early_stopping: 0 # 0 - no early stopping
  ignore_background_epochs: 0 # background images are not used for N epochs in train set
  num_workers: 12

  ### Validation ###
  conf_thresh: 0.5
  iou_thresh: 0.5

  ### EMA ###
  use_ema: True # use exponential moving average model
  ema_momentum: 0.9998

  ### Optimizer and Scheduler ###
  base_lr: ${train.lrs.${model_name}.base_lr}
  backbone_lr: ${train.lrs.${model_name}.backbone_lr}
  cycler_pct_start: 0.1
  weight_decay: 0.000125
  betas: [0.9, 0.999]
  label_smoothing: 0.0

  ### Augs ###
  mosaic_augs:
    mosaic_prob: 0.8
    no_mosaic_epochs: 5
    mosaic_scale: [0.5, 1.5]
    degrees: 0.0 # not recommended if bbox precision is critical
    translate: 0.2
    shear: 2.0

  augs:
    rotation_degree: 10 # maximum +- rotation
    rotation_p: 0.0 # probability of the rotation (with above degree)
    multiscale_prob: 0.0
    rotate_90: 0.05
    left_right_flip: 0.3
    up_down_flip: 0.0
    to_gray: 0.01
    blur: 0.01
    gamma: 0.02
    brightness: 0.02
    noise: 0.01
    coarse_dropout: 0.0

  ### Reproducibility ###
  seed: 42
  cudnn_fixed: False

  ### Recommended learning rates ###
  lrs:
    n:
      backbone_lr: 0.0004
      base_lr: 0.0008
    s:
      backbone_lr: 0.00006  # can setup up to 0.0002
      base_lr: 0.00025  # 0.0004
    m:
      backbone_lr: 0.00002  # 0.000025
      base_lr: 0.00015  # 0.00025
    l:
      backbone_lr: 0.00000625 # 0.0000125
      base_lr: 0.000125 # 0.00025
    x:
      backbone_lr: 0.0000015  # 0.0000025
      base_lr: 0.0001  # 0.00025


split:
  ignore_negatives: False # only use images with labels
  shuffle: True
  train_split: 0.85
  val_split: 0.15 # test_split = 1 - train_split - val_split


export: # TensorRT must be done on the inference device
  half: False # torch, tensorrt
  max_batch_size: 1 # torch, tensorrt
  dynamic_input: False # torch, openvino cpu only


infer:
  to_crop: True  # if True - saves crops of detected objects
  paddings: # if int - amount of pixes, if float - percentage of image size
    w: 0.05
    h: 0.05


### service ###
defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

now_dir: &nowdir ${now:%Y-%m-%d}
